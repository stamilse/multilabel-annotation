{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. length = low,medium,high\n",
    "1. subject/object/verb/adverb/adjective/abbrevation = yes,no\n",
    "1. voice = active,passive\n",
    "1. type of sentences = immperative, interrogative, exclamative, declarative, negative, affirmative etc.\n",
    "1. figures of speech = metaphor, simili, personification, idiom\n",
    "1. Tense = present, past, simple, participle, etc.\n",
    "1. Code Switching = yes, no\n",
    "1. Typos = yes, no\n",
    "1. Emojis = yes, no\n",
    "1. Homoglyphs = yes, no\n",
    "1. Special Characters = yes, no\n",
    "1. Grammatical Correctness = yes, no\n",
    "1. Clause = Dependent, independent, subordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Nice Place!\n",
      "length is low\n",
      "1: The room was nice/clean.\n",
      "length is low\n",
      "2: The location was great-connected to the mall.\n",
      "length is low\n",
      "3: The lobby was a bit congested and loud.\n",
      "length is low\n",
      "4: Great Hotel/Location\n",
      "length is low\n",
      "5: I got a great rate on priceline for this hotel.\n",
      "length is low\n",
      "6: Because I am a frequent Starwood (Shertaon Hotel Chain) guest, we were assigned a room in the North Tower for the Starwood Preferred Guests.\n",
      "length is medium\n",
      "7: The hallways don't look in very good shape, but the room was large, clean, and very comfortable.\n",
      "length is medium\n",
      "8: The hotel is well situated.\n",
      "length is low\n",
      "9: You can eat in the Prudential Center mall at the Au Bon Pan or the Marche for breakfast.\n",
      "length is low\n",
      "10: Much cheaper than in the hotel.\n",
      "length is low\n",
      "11: Plus, the mall has Legal Seafoods - one of the best seafood restaurants in Boston.\n",
      "length is low\n",
      "12: You can get to the T (Prudential Center) stop without going outside.\n",
      "length is medium\n",
      "13: The Upscale shops on Boylston is very close and the Boston Common is about one mile walk if you don't want to take the T.I would stay there again at twice the price.\n",
      "length is high\n",
      "14: Good location!\n",
      "length is low\n",
      "15: We stayed in this hotel on Aug 23-25 and Sept 6-7 again due to some business.\n",
      "length is low\n",
      "16: The first time we stayed in the North Tower with 2 double beds.\n",
      "length is low\n",
      "17: The room is very nice and the washroom is a bit old.\n",
      "length is low\n",
      "18: We asked for the south tower the second time after seeing the other's comments on tripadvisor.\n",
      "length is low\n",
      "19: Again we have 2 double beds and the room is a bit larger that the one in North tower.\n",
      "length is low\n",
      "20: The bathroom is new except the bathtub and the wall tile (They must want to save money by renovating only half of the bathroom).\n",
      "length is medium\n",
      "21: Overall, the north tower has nicer elevator and hallways, but the size of the room can vary.\n",
      "length is medium\n",
      "22: It is a nice hotel at a good location.\n",
      "length is low\n",
      "23: Nice hotel cons: parking is pricey (separate from hotel).\n",
      "length is low\n",
      "24: Service is typical of huge convention style hotel.\n",
      "length is low\n",
      "25: Pool/fitness center requires fee and looks pretty crowded.\n",
      "length is low\n",
      "26: Pros: good location.\n",
      "length is low\n",
      "27: Right next to big mall with nice stores and food.\n",
      "length is low\n",
      "28: Walking distance to Newbury street (great stores, and food - high end).\n",
      "length is low\n",
      "29: Very close to Charles R., and MIT/Harvard - good for a morning jog from the hotel)!\n",
      "length is low\n",
      "30: Decent expedia pricing.\n",
      "length is low\n",
      "31: Quilt and linens were new, but room itself had a dated look.\n",
      "length is medium\n",
      "32: 1King room itself is quite large.\n",
      "length is low\n",
      "33: Definitely a high usage place as nothing is really super-clean.\n",
      "length is low\n",
      "34: One of the worst I stayed at this hotel for a conference in June 03.\n",
      "length is low\n",
      "35: The first room I got, which was tiny, had a broken toilet.\n",
      "length is medium\n",
      "36: When I attempted to call the front desk I discovered the phones didn't work!\n",
      "length is medium\n",
      "37: The second room I got still had problems.\n",
      "length is low\n",
      "38: The cold-water faucet in the bathroom sink didn't work, the towel rack in the bathroom had fallen off the wall and was lying on the floor under the sink, there were suspicious stains on the comforter, all the channels on the TV were fuzzy and the Internet connection was flaky (they actually tried to bill me for a service I didn't use).\n",
      "length is high\n",
      "39: The service staff was slow to respond and down right rude in some cases.\n",
      "length is low\n",
      "40: Finally it's been two months and they won't credit me the airline miles for my stay.\n",
      "length is medium\n",
      "41: This was one of the worst hotels I've ever had the misfortune to stay at.\n",
      "length is medium\n"
     ]
    }
   ],
   "source": [
    "def calcVerb(num, sent):\n",
    "    c=0\n",
    "    for word in sent:\n",
    "        if(word.pos_=='VERB'):\n",
    "            c=c+1\n",
    "    if(c<3):\n",
    "        print(\"length is low\")\n",
    "    elif(c<6):\n",
    "        print(\"length is medium\")\n",
    "    else:\n",
    "        print(\"length is high\")\n",
    "\n",
    "text = \"Nice Place! The room was nice/clean.\\\n",
    "The location was great-connected to the mall.\\\n",
    "The lobby was a bit congested and loud.\\\n",
    "Great Hotel/Location I got a great rate on priceline for this hotel.\\\n",
    "Because I am a frequent Starwood (Shertaon Hotel Chain) guest, we were assigned a room in the North Tower for the Starwood Preferred Guests.\\\n",
    "The hallways don't look in very good shape, but the room was large, clean, and very comfortable.\\\n",
    "The hotel is well situated.\\\n",
    "You can eat in the Prudential Center mall at the Au Bon Pan or the Marche for breakfast.\\\n",
    "Much cheaper than in the hotel.\\\n",
    "Plus, the mall has Legal Seafoods - one of the best seafood restaurants in Boston.\\\n",
    "You can get to the T (Prudential Center) stop without going outside.\\\n",
    "The Upscale shops on Boylston is very close and the Boston Common is about one mile walk if you don't want to take the T.\\\n",
    "I would stay there again at twice the price. Good location! \\\n",
    "We stayed in this hotel on Aug 23-25 and Sept 6-7 again due to some business.\\\n",
    "The first time we stayed in the North Tower with 2 double beds.\\\n",
    "The room is very nice and the washroom is a bit old. We asked for the south tower the second time after seeing the other's comments on tripadvisor.\\\n",
    "Again we have 2 double beds and the room is a bit larger that the one in North tower.\\\n",
    "The bathroom is new except the bathtub and the wall tile \\\n",
    "(They must want to save money by renovating only half of the bathroom). \\\n",
    "Overall, the north tower has nicer elevator and hallways, but the size of the room can vary.\\\n",
    "It is a nice hotel at a good location.\\\n",
    "Nice hotel cons: parking is pricey (separate from hotel). \\\n",
    "Service is typical of huge convention style hotel.\\\n",
    "Pool/fitness center requires fee and looks pretty crowded.\\\n",
    "Pros: good location. Right next to big mall with nice stores and food.\\\n",
    "Walking distance to Newbury street (great stores, and food - high end). \\\n",
    "Very close to Charles R., and MIT/Harvard - good for a morning jog from the hotel)! \\\n",
    "Decent expedia pricing.\\\n",
    "Quilt and linens were new, but room itself had a dated look. \\\n",
    "1King room itself is quite large.\\\n",
    "Definitely a high usage place as nothing is really super-clean.\\\n",
    "One of the worst I stayed at this hotel for a conference in June 03. \\\n",
    "The first room I got, which was tiny, had a broken toilet.\\\n",
    "When I attempted to call the front desk I discovered the phones didn't work! \\\n",
    "The second room I got still had problems.\\\n",
    "The cold-water faucet in the bathroom sink didn't work, the towel rack in the bathroom had fallen off the wall and was lying on the floor under the sink, there were suspicious stains on the comforter, all the channels on the TV were fuzzy and the Internet connection was flaky (they actually tried to bill me for a service I didn't use). \\\n",
    "The service staff was slow to respond and down right rude in some cases.\\\n",
    "Finally it's been two months and they won't credit me the airline miles for my stay.\\\n",
    "This was one of the worst hotels I've ever had the misfortune to stay at.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "        \n",
    "for num,sentence in enumerate(doc.sents):\n",
    "    print(f'{num}: {sentence}')\n",
    "    calcVerb(num,sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Voice \n",
    "> (incomplete)\n",
    "\n",
    "If a clause has all of the following, then it is in the passive voice:\n",
    "- A form of an auxiliary verb (usually be or get)\n",
    "- The past participle of a transitive verb\n",
    "- No direct object\n",
    "- The subject of the verb phrase is the entity undergoing an action or having its state changed <br>\n",
    "Example: The documents were printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences found:  0\n",
      "Active\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\"\"\"\n",
    "Active = \"Harry ate six shrimp at dinner.\\\n",
    "Beautiful giraffes roam the savannah.\\\n",
    "Sue changed the flat tire.\\\n",
    "We are going to watch a movie tonight.\\\n",
    "I ran the obstacle course in record time.\\\n",
    "The crew paved the entire stretch of highway.\\\n",
    "Mom read the novel in one day.\\\n",
    "The critic wrote a scathing review.\\\n",
    "I will clean the house every Saturday.\\\n",
    "The staff is required to watch a safety video every year.\\\n",
    "She faxed her application for a new job.\\\n",
    "Tom painted the entire house.\\\n",
    "The teacher always answers the students’ questions.\\\n",
    "The choir really enjoys that piece.\\\n",
    "Who taught you to ski?\\\n",
    "The forest fire destroyed the whole suburb.\\\n",
    "The two kings are signing the treaty.\\\n",
    "The cleaning crew vacuums and dusts the office every night.\\\n",
    "Larry generously donated money to the homeless shelter.\\\n",
    "No one responded to my sales ad.\\\n",
    "The wedding planner is making all the reservations.\\\n",
    "Susan will bake two dozen cupcakes for the bake sale.\\\n",
    "The science class viewed the comet.\\\n",
    "Who ate the last cookie?\\\n",
    "Alex posted the video on Facebook.\\\n",
    "The director will give you instructions.\\\n",
    "Thousands of tourists view the Grand Canyon every year.\\\n",
    "The homeowners remodeled the house to help it sell.\\\n",
    "The team will celebrate their victory tomorrow.\\\n",
    "The saltwater eventually corroded the metal beams.\\\n",
    "The kangaroo carried her baby in her pouch.\\\n",
    "Some people raise sugar cane in Hawaii.\\\n",
    "He buys a camera.\\\n",
    "She drinks water.\\\n",
    "I know him.\\\n",
    "Water fills a tub.\\\n",
    "Women are not treated as equals.\\\n",
    "John has to study all afternoon.\\\n",
    "John is a good student.\\\n",
    "The dragon has scorched the metropolis with his fiery breath.\\\n",
    "After suitors invaded her house, Penelope had to think of ways to delay her remarriage.\\\n",
    "The Lao People’s Revolutionary Party set up a new system of drug control laws.\\\n",
    "Research points to heart disease as the leading cause of death in the United States.\\\n",
    "The surgeon positions the balloon in an area of blockage and inflates it.\\\n",
    "James writes the letters.\\\n",
    "James wrote the letters.\\\n",
    "James is writing the letters.\\\n",
    "James has written the letters.\\\n",
    "James is going to write the letters.\\\n",
    "James will write the letters.\\\n",
    "James was writing the letters.\\\n",
    "The scientists had found the cure, but it was too late.\\\n",
    "The scientists will have found a cure by then.\\\n",
    " I keep the butter in the fridge.\\\n",
    "John is keeping my house tidy.\\\n",
    "Mary kept her schedule meticulously.\\\n",
    "The theater was keeping a seat for you.\\\n",
    "I have kept all your old letters.\\\n",
    "He had kept up his training regimen for a month.\\\n",
    "Mark will keep the ficus.\\\n",
    "If you told me, I would keep your secret.\\\n",
    "I would have kept your bicycle here if you had left it with me.\\\n",
    "She wants to keep the book.\\\n",
    "Judy was happy to have kept the puppy.\\\n",
    "I have a feeling that you may be keeping a secret.\\\n",
    "Having kept the bird in a cage for so long, Jade wasn't sure it could survive in the wild.\\\n",
    "Guests might not play chess.\\\n",
    "He might meet Dewi.\\\n",
    "Dewi must not open the gate every morning.\\\n",
    "He must finish his duty in a week.\\\n",
    "I may not buy the computer.\\\n",
    "He may sell the house.\\\n",
    "May I buy the computer?\\\n",
    "Risky can not buy this car every time.\\\n",
    "She can sell the car every time.\\\n",
    "Can she play a violin?\"\n",
    "\n",
    "Passive = \"A camera is bought by him.\\\n",
    "Water is drunk by her.\\\n",
    "He is known to me.\\\n",
    "A tub is filled with water.\\\n",
    "Sugar is sold in kilograms.\\\n",
    "There is a considerable range of expertise demonstrated by the spam senders.\\\n",
    "It was determined by the committee that the report was inconclusive.\\\n",
    "We were invited by our neighbors to attend their party.\\\n",
    "Groups help participants realize that most of their problems and secrets are shared by others in the group.\\\n",
    "The proposed initiative will be bitterly opposed by abortion rights groups.\\\n",
    "Minor keys, modal movement, and arpeggios are shared by both musical traditions.\\\n",
    "In this way, the old religion was able to survive the onslaught of new ideas until the old gods were finally displaced by Christianity.\\\n",
    "First the apples are picked, then they are cleaned, and finally they’re packed and shipped to the market.\\\n",
    "New York is considered the most diverse city in the U.S.\\\n",
    "It is believed that Amelia Earhart’s plane crashed in Pacific Ocean.\\\n",
    "Hungarian is seen as one of the world’s most difficult languages to learn.\\\n",
    "Skin cancers are thought to be caused by excessive exposure to the sun.\\\n",
    "George Washington was elected president in 1788.\\\n",
    "Two people were killed in a drive-by shooting on Friday night.\\\n",
    "Ten children were injured when part of the school roof collapsed.\\\n",
    "I was hit by the dodgeball.\\\n",
    "The metropolis has been scorched by the dragon’s fiery breath.\\\n",
    "When her house was invaded, Penelope had to think of ways to delay her remarriage.\\\n",
    "A new system of drug control laws was set up.\\\n",
    "Heart disease is considered the leading cause of death in the United States.\\\n",
    "The balloon is positioned in an area of blockage and is inflated.\\\n",
    "The Exxon Company accepts that a few gallons might have been spilled.\\\n",
    "100 votes are required to pass the bill.\\\n",
    "Over 120 different contaminants have been dumped into the river.\\\n",
    "Baby Sophia was delivered at 3:30 a.m. yesterday.\\\n",
    "The letters are written by James.\\\n",
    "The letters were written by James.\\\n",
    "The letters are being written by James.\\\n",
    "The letters have been written by James.\\\n",
    "The letters are going to be written by James.\\\n",
    "The letters will be written by James.\\\n",
    "The letters were being written by James.\\\n",
    "The cure had been found, but it was too late.\\\n",
    "A cure will have been found by then.\\\n",
    "Mistakes were made.\\\n",
    "The butter is kept in the fridge.\\\n",
    "My house is being kept tidy.\\\n",
    "Mary's schedule was kept meticulously.\\\n",
    "A seat was being kept for you.\\\n",
    "All your old letters have been kept.\\\n",
    "His training regimen had been kept up for a month.\\\n",
    "The ficus will be kept.\\\n",
    "If you told me, your secret would be kept.\\\n",
    "Your bicycle would have been kept here if you had left it with me.\\\n",
    "The book wants to be kept.\\\n",
    "The puppy was happy to have been kept.\\\n",
    "I have a feeling that a secret may be being kept.\\\n",
    "The bird, having been kept in a cage for so long, might not survive in the wild.\\\n",
    "The car can be sold by her every time.\\\n",
    "Can a violin be played by her?\\\n",
    "The house may be sold by him.\\\n",
    "May the computer be bought by me?\\\n",
    "The computer may not be bought by me.\\\n",
    "His duty must be finished by him in a week.\\\n",
    "The gate must not be opened by Dewi every morning.\\\n",
    "Dewi might be met by him.\\\n",
    "Chess might not be played guests.\\\n",
    "At dinner, six shrimp were eaten by Harry.\\\n",
    "The savannah is roamed by beautiful giraffes.\\\n",
    "The flat tire was changed by Sue.\\\n",
    "A movie is going to be watched by us tonight.\\\n",
    "The obstacle course was run by me in record time.\\\n",
    "The entire stretch of highway was paved by the crew.\\\n",
    "The novel was read by Mom in one day.\\\n",
    "A scathing review was written by the critic.\\\n",
    "The house will be cleaned by me every Saturday.\\\n",
    "A safety video will be watched by the staff every year.\\\n",
    "The application for a new job was faxed by her.\\\n",
    "The entire house was painted by Tom.\\\n",
    "The students’ questions are always answered by the teacher.\\\n",
    "That piece is really enjoyed by the choir.\\\n",
    "By whom were you taught to ski?\\\n",
    "The whole suburb was destroyed by the forest fire.\\\n",
    "The treaty is being signed by the two kings.\\\n",
    "Every night the office is vacuumed and dusted by the cleaning crew.\\\n",
    "Money was generously donated to the homeless shelter by Larry.\\\n",
    "My sales ad was not responded to by anyone.\\\n",
    "All the reservations will be made by the wedding planner.\\\n",
    "For the bake sale, two dozen cookies will be baked by Susan.\\\n",
    "The comet was viewed by the science class.\\\n",
    "The video was posted on Facebook by Alex.\\\n",
    "Instructions will be given to you by the director.\\\n",
    "The Grand Canyon is viewed by thousands of tourists every year.\\\n",
    "The house was remodeled by the homeowners to help it sell.\\\n",
    "The victory will be celebrated by the team tomorrow.\\\n",
    "The metal beams were eventually corroded by the saltwater.\\\n",
    "The baby was carried by the kangaroo in her pouch.\\\n",
    "The last cookie was eaten by whom?\"\n",
    "\"\"\"\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "text = \"Women are not treated as equals.\"\n",
    "doc = nlp(text)\n",
    "#sents = list(doc.sents)\n",
    "#print(\"Number of Sentences = \",len(sents))\n",
    "\"\"\"\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.dep_,token.tag_, end = \" \")\n",
    "    print()\n",
    "\"\"\"\n",
    "passive_rule = [{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]\n",
    "matcher.add('Passive',None,passive_rule)\n",
    "matches = matcher(doc)\n",
    "print(\"The number of sentences found: \",len(matches))\n",
    "if (len(matches) != 0):\n",
    "    print(\"Passive\")\n",
    "else:\n",
    "    print(\"Active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Emoji\n",
    "\n",
    "Using the python package \"spacymoji\"\n",
    ">`pip install spacymoji`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"assert doc[2:5]._.has_emoji == True\\nassert doc[0]._.is_emoji == False\\nassert doc[4]._.is_emoji == True\\nassert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\\nassert len(doc._.emoji) == 2\\nassert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "emoji = Emoji(nlp)\n",
    "#nlp.add_pipe(emoji, first=True)\n",
    "doc = nlp(u\"This is a test 😻 👍🏿\")\n",
    "#assert doc._.has_emoji == True\n",
    "\"\"\"assert doc[2:5]._.has_emoji == True\n",
    "assert doc[0]._.is_emoji == False\n",
    "assert doc[4]._.is_emoji == True\n",
    "assert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\n",
    "assert len(doc._.emoji) == 2\n",
    "assert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Using an LSTM sentiment classification model trained using Keras in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows how to use an LSTM sentiment classification model trained\n",
    "using Keras in spaCy. spaCy splits the document into sentences, and each\n",
    "sentence is classified using the LSTM. The scores for the sentences are then\n",
    "aggregated to give the document score. This kind of hierarchical model is quite\n",
    "difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras\n",
    "example on this dataset performs quite poorly, because it cuts off the documents\n",
    "so that they're a fixed size. This hurts review accuracy a lot, because people\n",
    "often summarise their rating in the final sentence\n",
    "\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        with (path / \"config.json\").open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / \"model\").open(\"rb\") as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype=\"int32\")\n",
    "\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype=\"int32\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    dev_texts,\n",
    "    dev_labels,\n",
    "    lstm_shape,\n",
    "    lstm_settings,\n",
    "    lstm_optimizer,\n",
    "    batch_size=100,\n",
    "    nb_epoch=5,\n",
    "    by_sentence=True,\n",
    "):\n",
    "\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape[\"max_length\"])\n",
    "    dev_X = get_features(dev_docs, lstm_shape[\"max_length\"])\n",
    "    model.fit(\n",
    "        train_X,\n",
    "        train_labels,\n",
    "        validation_data=(dev_X, dev_labels),\n",
    "        epochs=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape[\"max_length\"],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True,\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape[\"nr_hidden\"], use_bias=False)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                shape[\"nr_hidden\"],\n",
    "                recurrent_dropout=settings[\"dropout\"],\n",
    "                dropout=settings[\"dropout\"],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(shape[\"nr_class\"], activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=settings[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    for subdir, label in ((\"pos\", 1), (\"neg\", 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\n",
    "    if limit >= 1:\n",
    "        examples = examples[:limit]\n",
    "    return zip(*examples)  # Unzips into two lists\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    train_dir=(\"Location of training file or directory\"),\n",
    "    dev_dir=(\"Location of development file or directory\"),\n",
    "    model_dir=(\"Location of output model directory\",),\n",
    "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
    "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
    "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
    "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
    "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
    "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
    "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
    "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(\n",
    "    model_dir=None,\n",
    "    train_dir=None,\n",
    "    dev_dir=None,\n",
    "    is_runtime=False,\n",
    "    nr_hidden=64,\n",
    "    max_length=100,  # Shape\n",
    "    dropout=0.5,\n",
    "    learn_rate=0.001,  # General NN config\n",
    "    nb_epoch=5,\n",
    "    batch_size=256,\n",
    "    nr_examples=-1,\n",
    "):  # Training params\n",
    "    if model_dir is not None:\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if train_dir is None or dev_dir is None:\n",
    "        imdb_data = thinc.extra.datasets.imdb()\n",
    "    if is_runtime:\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir)\n",
    "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        if train_dir is None:\n",
    "            train_texts, train_labels = zip(*imdb_data[0])\n",
    "        else:\n",
    "            print(\"Read data\")\n",
    "            train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)\n",
    "        train_labels = numpy.asarray(train_labels, dtype=\"int32\")\n",
    "        dev_labels = numpy.asarray(dev_labels, dtype=\"int32\")\n",
    "        lstm = train(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            dev_texts,\n",
    "            dev_labels,\n",
    "            {\"nr_hidden\": nr_hidden, \"max_length\": max_length, \"nr_class\": 1},\n",
    "            {\"dropout\": dropout, \"lr\": learn_rate},\n",
    "            {},\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / \"model\").open(\"wb\") as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / \"config.json\").open(\"w\") as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
