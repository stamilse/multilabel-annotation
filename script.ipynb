{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. length = low,medium,high\n",
    "1. subject/object/verb/adverb/adjective/abbrevation = yes,no\n",
    "1. voice = active,passive\n",
    "1. type of sentences = immperative, interrogative, exclamative, declarative, negative, affirmative etc.\n",
    "1. figures of speech = metaphor, simili, personification, idiom\n",
    "1. Tense = present, past, simple, participle, etc.\n",
    "1. Code Switching = yes, no\n",
    "1. Typos = yes, no\n",
    "1. Emojis = yes, no\n",
    "1. Homoglyphs = yes, no\n",
    "1. Special Characters = yes, no\n",
    "1. Grammatical Correctness = yes, no\n",
    "1. Clause = Dependent, independent, subordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.add_pipe(nlp.create_pipe('sentencizer')) # Ref : https://spacy.io/api/sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "d_text = \"\"\n",
    "with open('RC_2005-12.json','r') as data_file:\n",
    "    for line in data_file:\n",
    "        data = json.loads(line)\n",
    "        d_text = d_text + data['body'] + \"\\n\"\n",
    "        \n",
    "#print(d_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(d_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calcVerb(text):\n",
    "    c=0\n",
    "    for word in text:\n",
    "        if(word.pos_=='VERB'):\n",
    "            c=c+1\n",
    "    if(c<3):\n",
    "        print(\"length is low\")\n",
    "    elif(c<6):\n",
    "        print(\"length is medium\")\n",
    "    else:\n",
    "        print(\"length is high\")\n",
    "\n",
    "text = \"Nice Place! The room was nice/clean.\\\n",
    "The location was great-connected to the mall.\\\n",
    "The lobby was a bit congested and loud.\\\n",
    "Great Hotel/Location I got a great rate on priceline for this hotel.\\\n",
    "Because I am a frequent Starwood (Shertaon Hotel Chain) guest, we were assigned a room in the North Tower for the Starwood Preferred Guests.\\\n",
    "The hallways don't look in very good shape, but the room was large, clean, and very comfortable.\\\n",
    "The hotel is well situated.\\\n",
    "You can eat in the Prudential Center mall at the Au Bon Pan or the Marche for breakfast.\\\n",
    "Much cheaper than in the hotel.\\\n",
    "Plus, the mall has Legal Seafoods - one of the best seafood restaurants in Boston.\\\n",
    "You can get to the T (Prudential Center) stop without going outside.\\\n",
    "The Upscale shops on Boylston is very close and the Boston Common is about one mile walk if you don't want to take the T.\\\n",
    "I would stay there again at twice the price. Good location! \\\n",
    "We stayed in this hotel on Aug 23-25 and Sept 6-7 again due to some business.\\\n",
    "The first time we stayed in the North Tower with 2 double beds.\\\n",
    "The room is very nice and the washroom is a bit old. We asked for the south tower the second time after seeing the other's comments on tripadvisor.\\\n",
    "Again we have 2 double beds and the room is a bit larger that the one in North tower.\\\n",
    "The bathroom is new except the bathtub and the wall tile \\\n",
    "(They must want to save money by renovating only half of the bathroom). \\\n",
    "Overall, the north tower has nicer elevator and hallways, but the size of the room can vary.\\\n",
    "It is a nice hotel at a good location.\\\n",
    "Nice hotel cons: parking is pricey (separate from hotel). \\\n",
    "Service is typical of huge convention style hotel.\\\n",
    "Pool/fitness center requires fee and looks pretty crowded.\\\n",
    "Pros: good location. Right next to big mall with nice stores and food.\\\n",
    "Walking distance to Newbury street (great stores, and food - high end). \\\n",
    "Very close to Charles R., and MIT/Harvard - good for a morning jog from the hotel)! \\\n",
    "Decent expedia pricing.\\\n",
    "Quilt and linens were new, but room itself had a dated look. \\\n",
    "1King room itself is quite large.\\\n",
    "Definitely a high usage place as nothing is really super-clean.\\\n",
    "One of the worst I stayed at this hotel for a conference in June 03. \\\n",
    "The first room I got, which was tiny, had a broken toilet.\\\n",
    "When I attempted to call the front desk I discovered the phones didn't work! \\\n",
    "The second room I got still had problems.\\\n",
    "The cold-water faucet in the bathroom sink didn't work, the towel rack in the bathroom had fallen off the wall and was lying on the floor under the sink, there were suspicious stains on the comforter, all the channels on the TV were fuzzy and the Internet connection was flaky (they actually tried to bill me for a service I didn't use). \\\n",
    "The service staff was slow to respond and down right rude in some cases.\\\n",
    "Finally it's been two months and they won't credit me the airline miles for my stay.\\\n",
    "This was one of the worst hotels I've ever had the misfortune to stay at.\"\n",
    "        \n",
    "for num,sentence in enumerate(doc.sents):\n",
    "    print(f'{num}: {sentence}')\n",
    "    calcVerb(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. subject/object/verb/adverb/adjective/abbrevation = yes,no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5f1417840849>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'Startup companies create jobs and support innovation. Hilary supports entrepreneurship.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubject_verb_object_triples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textacy'"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "\n",
    "text = nlp(u'Startup companies create jobs and support innovation. Hilary supports entrepreneurship.')\n",
    "\n",
    "text_ext = textacy.extract.subject_verb_object_triples(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Type of Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPERATIVE SENTENCES\n",
    "matcher = Matcher(nlp.vocab)\n",
    "sent=(\"What Is a Declarative Sentence?\")\n",
    "document=nlp(sent)\n",
    "\n",
    "def func1(tagged):\n",
    "    if(tagged[-2]!=\"?\"):\n",
    "        if(tagged[1]==\"VB\" or tagged[1]==\"MD\"):\n",
    "            return True\n",
    "        \n",
    "tagged=tuple()\n",
    "for word in document:\n",
    "    tagged=tagged+((word.text,word.tag_))\n",
    "if func1(tagged):\n",
    "    print(\"imperative\")\n",
    "    \n",
    "\n",
    "#NEGATIVE\n",
    "def is_negative(matcher, doc, id, matches):\n",
    "    print(\"negative sentence\")\n",
    "matcher.add('ISNEG',is_negative,[{'DEP':'neg'}])\n",
    "matches = matcher(document)\n",
    "#SUBJECT,OBJECT,VERB\n",
    "for word in document:\n",
    "    if(word.dep_=='nsubj'):\n",
    "        print(\"subject: \",word)\n",
    "    if(word.pos_=='VERB'):\n",
    "        print(\"verb: \",word)\n",
    "    if(word.dep_=='dobj'):\n",
    "        print(\"object: \",word)\n",
    "    if(word.pos_=='ADV'):\n",
    "        print(\"Adverb: \",word)\n",
    "    if(word.pos_=='ADJ'):\n",
    "        print(\"Adjective: \",word)\n",
    "\n",
    "\n",
    "#EXCALMATIVE\n",
    "if(document[-1].pos_=='PUNCT'):\n",
    "    if(document[-1].text=='!'):\n",
    "        print(\"Exclamative sentence.\")\n",
    "\n",
    "\n",
    "#DECLARATIVE\n",
    "if(document[-1].pos_=='PUNCT'):\n",
    "    if(document[-1].text=='.'):\n",
    "        print(\"Declarative sentence.\")\n",
    "        \n",
    "\n",
    "#INTERROGATIVE\n",
    "if(document[-1].pos_=='PUNCT'):\n",
    "    if(document[-1].text=='?'):\n",
    "        print(\"Interrogative sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRESENT PERFECT CONTINUOUS\n",
    "from spacy.matcher import Matcher\n",
    "def on_matchppct(matcher, doc, id, matches):\n",
    "    print(\"PRESENT PERFECT CONTINUOUS\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PPCT',on_matchppct,[{\"LOWER\": \"has\"},{\"LOWER\": \"been\"},{'TAG':'VBG'}])\n",
    "matcher.add('PPCT',on_matchppct,[{'TAG':'MD','OP':'!'},{\"LOWER\": \"have\"},{\"LOWER\": \"been\"},{'TAG':'VBG'}])\n",
    "#PRESENT PERFECT\n",
    "def on_matchppt(matcher, doc, id, matches):\n",
    "    print(\"Present perfect\")\n",
    "matcher.add('PPT',on_matchppt,[{\"LOWER\": \"has\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'TAG':'VBN'},{'TAG':'VBG','OP':'!'}])\n",
    "matcher.add('PPT',on_matchppt,[{\"LOWER\": \"have\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'TAG':'VBN'},{'TAG':'VBG','OP':'!'}])\n",
    "#PRESENT CONTINUOUS\n",
    "def on_matchpc(matcher, doc, id, matches):\n",
    "        print(\"PRESENT CONTINUOUS\")\n",
    "matcher.add('PCTense',on_matchpc,[{\"LOWER\": \"is\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBG'}])\n",
    "matcher.add('PCTense',on_matchpc,[{\"LOWER\": \"am\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBG'}])\n",
    "matcher.add('PCTense',on_matchpc,[{\"LOWER\": \"are\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBG'}])\n",
    "#PAST PERFECT\n",
    "def on_match_pastperf(matcher, doc, id, matches):\n",
    "    print(\"PAST PERFECT\")\n",
    "matcher.add('PASTPERF',on_match_pastperf,[{\"TEXT\":\"had\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBN'},{'TAG':'VBG','OP':'!'}])\n",
    "#PAST CONTINUOUS\n",
    "def on_match_pastcont(matcher, doc, id,matches):\n",
    "    print(\"past continuous\")\n",
    "matcher.add('PASTCONT',on_match_pastcont,[{\"LOWER\":\"was\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBG'}])\n",
    "matcher.add('PASTCONT',on_match_pastcont,[{\"LOWER\":\"were\"},{'DEP':'neg','OP':'?'},{'DEP':'nsubj','OP':'?'},{'POS':'ADV','OP':'?'},{'TAG':'VBG'}])\n",
    "#PAST PERFECT CONTINUOUS\n",
    "def on_match_pastpc(matcher, doc, id, matches):\n",
    "    print(\"PAST PERFECT CONTINUOUS\")\n",
    "matcher.add('PASTPC',on_match_pastpc,[{\"LOWER\":\"had\"},{'DEP':'neg','OP':'?'},{\"LOWER\":\"been\"},{'TAG':'VBG'}])\n",
    "matcher.add('PASTPC',on_match_pastpc,[{\"LOWER\":\"had\"},{'DEP':'det','OP':'?'},{'DEP':'nsubj','OP':'?'},{\"LOWER\":\"been\"},{'TAG':'VBG'}])\n",
    "#FUTURE SIMPLE\n",
    "def on_match_futsim(matcher, doc, id, matches):\n",
    "    print(\"FUTURE SIMPLE\")\n",
    "matcher.add('FUTSIM',on_match_futsim,[{'TAG':'MD'},{'DEP':'nsubj','OP':'?'},{'DEP':'neg','OP':'?'},{'TAG':'VB'},{'DEP':'aux','OP':'!'}])\n",
    "#FUTURE CONTINUOUS\n",
    "def on_match_futcont(matcher, doc, id, matches):\n",
    "    print(\"FUTURE CONTINUOUS\")\n",
    "matcher.add('FUTCONT',on_match_futcont,[{'TAG':'MD'},{'DEP':'nsubj','OP':'?'},{'DEP':'neg','OP':'?'},{\"LOWER\":\"be\"},{'TAG':'VBG'}])\n",
    "#FUTURE PERFECT\n",
    "def on_match_futperf(matcher, doc, id, matches):\n",
    "    print(\"future perfect\")\n",
    "matcher.add('FUTPERF',on_match_futperf,[{'TAG':'MD'},{'DEP':'nsubj','OP':'?'},{'DEP':'neg','OP':'?'},{\"LOWER\":\"have\"},{'TAG':'VBN'},{'TAG':'VBG','OP':'!'}])\n",
    "#FUTURE PERFECT CONTINUOUS\n",
    "def on_match_futpc(matcher, doc, id, matches):\n",
    "    print(\"FUTURE PERFECT CONTINUOUS\")\n",
    "matcher.add('FUTPC',on_match_futpc,[{'TAG':'MD'},{'DEP':'nsubj','OP':'?'},{'DEP':'neg','OP':'?'},{\"LOWER\":\"have\"},{\"LOWER\":\"been\"},{'TAG':'VBG'}])\n",
    "\n",
    "def tense_form(text):\n",
    "    matches = matcher(text)\n",
    "\n",
    "dd = nlp(\"He will not buy a car. They will have been living in Paris for five years \")\n",
    "# ding = nlp(\"He has been reading the book for two hours.\")\n",
    "# dong = nlp(\"He has finished his homework.\")\n",
    "\n",
    "#sentence input\n",
    "for num,sentence in enumerate(dd.sents):\n",
    "    print(f'{num}: {sentence}')\n",
    "    s = sentence.as_doc()\n",
    "    tense_form(s)\n",
    "\n",
    "#tense_form(\"He has finished his homework.\")\n",
    "#tense_form(dong)\n",
    "#matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"He has been reading the book for two hours. He has finished his homework.\"\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"sentence : \",sentence)\n",
    "    doc = nlp(sentence)\n",
    "    tense_form(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Emoji\n",
    "\n",
    "Using the python package \"spacymoji\"\n",
    ">`pip install spacymoji`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "emoji = Emoji(nlp)\n",
    "#nlp.add_pipe(emoji, first=True)\n",
    "doc = nlp(u\"This is a test 😻 👍🏿\")\n",
    "#assert doc._.has_emoji == True\n",
    "\"\"\"assert doc[2:5]._.has_emoji == True\n",
    "assert doc[0]._.is_emoji == False\n",
    "assert doc[4]._.is_emoji == True\n",
    "assert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\n",
    "assert len(doc._.emoji) == 2\n",
    "assert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Using an LSTM sentiment classification model trained using Keras in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows how to use an LSTM sentiment classification model trained\n",
    "using Keras in spaCy. spaCy splits the document into sentences, and each\n",
    "sentence is classified using the LSTM. The scores for the sentences are then\n",
    "aggregated to give the document score. This kind of hierarchical model is quite\n",
    "difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras\n",
    "example on this dataset performs quite poorly, because it cuts off the documents\n",
    "so that they're a fixed size. This hurts review accuracy a lot, because people\n",
    "often summarise their rating in the final sentence\n",
    "\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        with (path / \"config.json\").open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / \"model\").open(\"rb\") as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype=\"int32\")\n",
    "\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype=\"int32\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    dev_texts,\n",
    "    dev_labels,\n",
    "    lstm_shape,\n",
    "    lstm_settings,\n",
    "    lstm_optimizer,\n",
    "    batch_size=100,\n",
    "    nb_epoch=5,\n",
    "    by_sentence=True,\n",
    "):\n",
    "\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape[\"max_length\"])\n",
    "    dev_X = get_features(dev_docs, lstm_shape[\"max_length\"])\n",
    "    model.fit(\n",
    "        train_X,\n",
    "        train_labels,\n",
    "        validation_data=(dev_X, dev_labels),\n",
    "        epochs=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape[\"max_length\"],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True,\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape[\"nr_hidden\"], use_bias=False)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                shape[\"nr_hidden\"],\n",
    "                recurrent_dropout=settings[\"dropout\"],\n",
    "                dropout=settings[\"dropout\"],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(shape[\"nr_class\"], activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=settings[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    for subdir, label in ((\"pos\", 1), (\"neg\", 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\n",
    "    if limit >= 1:\n",
    "        examples = examples[:limit]\n",
    "    return zip(*examples)  # Unzips into two lists\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    train_dir=(\"Location of training file or directory\"),\n",
    "    dev_dir=(\"Location of development file or directory\"),\n",
    "    model_dir=(\"Location of output model directory\",),\n",
    "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
    "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
    "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
    "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
    "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
    "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
    "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
    "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(\n",
    "    model_dir=None,\n",
    "    train_dir=None,\n",
    "    dev_dir=None,\n",
    "    is_runtime=False,\n",
    "    nr_hidden=64,\n",
    "    max_length=100,  # Shape\n",
    "    dropout=0.5,\n",
    "    learn_rate=0.001,  # General NN config\n",
    "    nb_epoch=5,\n",
    "    batch_size=256,\n",
    "    nr_examples=-1,\n",
    "):  # Training params\n",
    "    if model_dir is not None:\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if train_dir is None or dev_dir is None:\n",
    "        imdb_data = thinc.extra.datasets.imdb()\n",
    "    if is_runtime:\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir)\n",
    "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        if train_dir is None:\n",
    "            train_texts, train_labels = zip(*imdb_data[0])\n",
    "        else:\n",
    "            print(\"Read data\")\n",
    "            train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)\n",
    "        train_labels = numpy.asarray(train_labels, dtype=\"int32\")\n",
    "        dev_labels = numpy.asarray(dev_labels, dtype=\"int32\")\n",
    "        lstm = train(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            dev_texts,\n",
    "            dev_labels,\n",
    "            {\"nr_hidden\": nr_hidden, \"max_length\": max_length, \"nr_class\": 1},\n",
    "            {\"dropout\": dropout, \"lr\": learn_rate},\n",
    "            {},\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / \"model\").open(\"wb\") as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / \"config.json\").open(\"w\") as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
