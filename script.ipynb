{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. length = low,medium,high\n",
    "1. subject/object/verb/adverb/adjective/abbrevation = yes,no\n",
    "1. voice = active,passive\n",
    "1. type of sentences = immperative, interrogative, exclamative, declarative, negative, affirmative etc.\n",
    "1. figures of speech = metaphor, simili, personification, idiom\n",
    "1. Tense = present, past, simple, participle, etc.\n",
    "1. Code Switching = yes, no\n",
    "1. Typos = yes, no\n",
    "1. Emojis = yes, no\n",
    "1. Homoglyphs = yes, no\n",
    "1. Special Characters = yes, no\n",
    "1. Grammatical Correctness = yes, no\n",
    "1. Clause = Dependent, independent, subordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Voice \n",
    "> (incomplete)\n",
    "\n",
    "If a clause has all of the following, then it is in the passive voice:\n",
    "- A form of an auxiliary verb (usually be or get)\n",
    "- The past participle of a transitive verb\n",
    "- No direct object\n",
    "- The subject of the verb phrase is the entity undergoing an action or having its state changed <br>\n",
    "Example: The documents were printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences =  87\n",
      "det DT nsubjpass NN auxpass VBZ ROOT VBN agent IN pobj PRP punct . \n",
      "nsubj NN ROOT VBZ acomp JJ prep IN pobj PRP punct . \n",
      "nsubjpass PRP auxpass VBZ ROOT VBN prep IN pobj PRP punct . \n",
      "det DT nsubjpass NN auxpass VBZ ROOT VBN prep IN pobj NN punct . \n",
      "nsubjpass NN auxpass VBZ ROOT VBN prep IN pobj NNS punct . \n",
      "expl EX ROOT VBZ det DT amod JJ attr NN prep IN pobj NN acl VBN agent IN det DT compound NN pobj NNS punct . \n",
      "nsubjpass PRP auxpass VBD ROOT VBN agent IN det DT pobj NN mark IN det DT nsubj NN ccomp VBD acomp JJ punct . \n",
      "nsubjpass PRP auxpass VBD ROOT VBN agent IN poss PRP$ pobj NNS aux TO xcomp VB poss PRP$ dobj NN punct . \n",
      "nsubj NNS ROOT VBP nsubj NNS ccomp VB mark IN nsubjpass JJS prep IN poss PRP$ pobj NNS cc CC conj NNS auxpass VBP ccomp VBN agent IN pobj NNS prep IN det DT pobj NN punct . \n",
      "det DT amod VBN nsubjpass NN aux MD auxpass VB advmod RB ROOT VBN agent IN compound NN compound NNS pobj NNS punct . \n",
      "amod JJ nsubjpass NNS punct , amod JJ conj NN punct , cc CC conj NNS auxpass VBP ROOT VBN agent IN det DT amod JJ pobj NNS punct . \n",
      "prep IN det DT pobj NN punct , det DT amod JJ nsubj NN ROOT VBD acomp JJ aux TO xcomp VB det DT dobj NN prep IN amod JJ pobj NNS mark IN det DT amod JJ nsubjpass NNS auxpass VBD advmod RB advcl VBN agent IN pobj NNP punct . \n",
      "advmod RB det DT nsubjpass NNS auxpass VBP ccomp VBN punct , advmod RB nsubjpass PRP auxpass VBP ROOT VBN punct , cc CC advmod RB nsubj PRP advmod VBP conj VBD cc CC conj VBN prep IN det DT pobj NN punct . \n",
      "compound NNP nsubjpass NNP auxpass VBZ ROOT VBN det DT advmod RBS amod JJ nsubjpass NN prep IN det DT pobj NNP auxpass VBZ ccomp VBN mark IN compound NNP nmod NNP compound NNP nsubj NN ccomp VBD prep IN compound NNP pobj NNP punct . \n",
      "nsubjpass NNP auxpass VBZ ROOT VBN prep IN pobj CD prep IN det DT compound NN pobj NNP advmod RBS amod JJ appos NNS aux TO relcl VB punct . \n",
      "compound NN nsubjpass NNS auxpass VBP ROOT VBN aux TO auxpass VB xcomp VBN agent IN amod JJ pobj NN prep IN det DT pobj NN punct . \n",
      "compound NNP nsubjpass NNP auxpass VBD ROOT VBN nsubjpass NN prep IN pobj CD pobj NNS auxpass VBD relcl VBN prep IN det DT pobj NN punct HYPH prt NN pobj VBG prep IN compound NNP pobj NN punct . \n",
      "nummod CD nsubjpass NNS auxpass VBD ROOT VBN advmod WRB nsubj NN prep IN det DT compound NN pobj NN advcl VBD punct . \n",
      "nsubjpass PRP auxpass VBD ROOT VBN agent IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN aux VBZ auxpass VBN ROOT VBN agent IN det DT nmod NN nummod NNP amod JJ pobj NN punct . \n",
      "advmod WRB poss PRP$ nsubjpass NN auxpass VBD advcl VBN punct , nsubj NNP ROOT VBD aux TO xcomp VB prep IN pobj NNS aux TO relcl VB poss PRP$ dobj NN punct . \n",
      "det DT amod JJ nsubjpass NN prep IN compound NN compound NN pobj NNS auxpass VBD ROOT VBN prt RP punct . \n",
      "compound NN nsubjpass NN auxpass VBZ ROOT VBN det DT amod JJ oprd NN prep IN pobj NN prep IN det DT compound NNP pobj NNP punct . \n",
      "det DT nsubjpass NN auxpass VBZ ROOT VBN prep IN det DT pobj NN prep IN pobj NN cc CC auxpass VBZ conj JJ punct . \n",
      "det DT compound NNP nsubj NNP ROOT VBZ mark IN det DT amod JJ nsubj NNS aux MD aux VB ccomp VBN nummod NNP attr NNS auxpass VBP ccomp VBN aux TO xcomp VB det DT dobj NN punct . \n",
      "quantmod IN nummod CD amod JJ nsubjpass NNS aux VBP auxpass VBN ROOT VBN prep IN det DT pobj NN punct . \n",
      "compound NNP nsubjpass NNP auxpass VBD ROOT VBN prep IN nummod CD pobj NN npadvmod NN punct . \n",
      "det DT nsubjpass NNS auxpass VBP ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS auxpass VBD ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS aux VBP auxpass VBG ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS aux VBP auxpass VBN ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubj NNS aux VBP ROOT VBG aux TO auxpass VB xcomp VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS aux MD auxpass VB ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS aux VBD auxpass VBG ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NN aux VBD auxpass VBN ROOT VBN punct , cc CC nsubj PRP conj VBD advmod RB acomp JJ punct . \n",
      "det DT nsubjpass NN aux MD aux VB auxpass VBN ROOT VBN agent IN pcomp RB punct . \n",
      "nsubjpass NNS auxpass VBD ROOT VBN punct . \n",
      "det DT nsubjpass NN auxpass VBZ ROOT VBN prep IN det DT pobj NN punct . \n",
      "poss PRP$ nsubjpass NN aux VBZ auxpass VBG ROOT VBN oprd JJ punct . \n",
      "poss NNP case POS nsubjpass NN auxpass VBD ROOT VBN advmod RB punct . \n",
      "det DT nsubjpass NN aux VBD auxpass VBG ROOT VBN dative IN pobj PRP punct . \n",
      "predet DT poss PRP$ amod JJ nsubjpass NNS aux VBP auxpass VBN ROOT VBN punct . \n",
      "poss PRP$ compound NN nsubjpass NN aux VBD auxpass VBN ROOT VBN prt RP prep IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN aux MD auxpass VB ROOT VBN punct . \n",
      "mark IN nsubj PRP advcl VBD dobj PRP punct , poss PRP$ nsubjpass NN aux MD auxpass VB ROOT VBN punct . \n",
      "poss PRP$ nsubjpass NN aux MD aux VB auxpass VBN ROOT VBN advmod RB mark IN nsubj PRP aux VBD advcl VBN dobj PRP prep IN pobj PRP punct . \n",
      "det DT nsubj NN ROOT VBZ aux TO auxpass VB xcomp VBN punct . \n",
      "det DT nsubj JJ ROOT VBD acomp JJ aux TO aux VB auxpass VBN xcomp VBN punct . \n",
      "nsubj PRP ROOT VBP det DT dobj NN mark IN det DT nsubjpass NN aux MD aux VB auxpass VBG acl VBN punct . \n",
      "det DT nsubjpass NN punct , aux VBG auxpass VBN ccomp VBN prep IN det DT pobj NN prep IN advmod RB pcomp RB punct , aux MD neg RB ROOT VB prep IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN aux MD auxpass VB ROOT VBN agent IN pobj PRP det DT npadvmod NN punct . \n",
      "aux MD det DT nsubjpass NN auxpass VB csubjpass VBN agent IN nummod CD pobj NN aux MD auxpass VB ROOT VBN agent IN pobj PRP punct . \n",
      "aux MD det DT nsubjpass NN auxpass VB ROOT VBN agent IN compound , pobj NN aux MD neg RB auxpass VB conj VBN agent IN pobj PRP punct . \n",
      "poss PRP$ nsubjpass NN aux MD auxpass VB ROOT VBN agent IN pobj PRP prep IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN aux MD neg RB auxpass VB ROOT VBN agent IN pobj NNP det DT npadvmod NN punct . \n",
      "nsubjpass NNP aux MD auxpass VB ROOT VBN agent IN pobj PRP punct . \n",
      "nsubjpass NN aux MD neg RB auxpass VB ROOT VBN oprd NNS punct . \n",
      "prep IN pobj NN punct , nummod CD nsubjpass NN auxpass VBD ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NN auxpass VBZ ROOT VBN agent IN amod JJ pobj NN punct . \n",
      "det DT amod JJ nsubjpass NN auxpass VBD ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubj NN aux VBZ ROOT VBG aux TO auxpass VB xcomp VBN agent IN pobj PRP npadvmod NN punct . \n",
      "det DT compound NN nsubjpass NN auxpass VBD ROOT VBN agent IN pobj PRP prep IN compound NN pobj NN punct . \n",
      "det DT amod JJ nsubjpass NN prep IN pobj NN auxpass VBD ROOT VBN agent IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN auxpass VBD ROOT VBN agent IN pobj NNP prep IN nummod CD pobj NN punct . \n",
      "det DT amod JJ nsubjpass NN auxpass VBD ROOT VBN agent IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN aux MD auxpass VB ROOT VBN agent IN pobj PRP det DT npadvmod NNP punct . \n",
      "det DT compound NN nsubjpass NN aux MD auxpass VB ROOT VBN agent IN det DT pobj NN det DT npadvmod NN punct . \n",
      "det DT nsubjpass NN prep IN det DT amod JJ pobj NN auxpass VBD ROOT VBN agent IN pobj PRP punct . \n",
      "det DT amod JJ nsubjpass NN auxpass VBD ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NNS compound POS nsubjpass NNS auxpass VBP advmod RB ROOT VBN agent IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN auxpass VBZ advmod RB ROOT VBN agent IN det DT pobj NN punct . \n",
      "prep IN pobj WP auxpass VBD nsubj PRP ROOT VBN prep IN det DT amod JJ nsubjpass NN auxpass VBD ccomp VBN agent IN det DT compound NN pobj NN punct . \n",
      "det DT nsubjpass NN aux VBZ auxpass VBG ROOT VBN agent IN det DT nummod CD pobj NNS punct . \n",
      "det DT npadvmod NN det DT nsubjpass NN auxpass VBZ ROOT VBN cc CC conj VBN agent IN det DT compound NN pobj NN punct . \n",
      "nsubjpass NN auxpass VBD advmod RB ROOT VBN prep IN det DT amod JJ pobj NN agent IN pobj NNP punct . \n",
      "poss PRP$ compound NNS nsubjpass NN auxpass VBD neg RB ROOT VBN prep IN agent IN pobj NN punct . \n",
      "predet PDT det DT nsubjpass NNS aux MD auxpass VB ROOT VBN agent IN det DT compound NN pobj NN punct . \n",
      "prep IN det DT compound NN pobj NN punct , nummod CD compound NN nsubjpass NNS aux MD auxpass VB ROOT VBN agent IN pobj NNP punct . \n",
      "det DT nsubjpass NN auxpass VBD ROOT VBN agent IN det DT compound NN pobj NN punct . \n",
      "det DT nsubjpass NN auxpass VBD ROOT VBN prep IN pobj NNP agent IN pobj NNP punct . \n",
      "nsubjpass NNS aux MD auxpass VB ROOT VBN dative IN pobj PRP agent IN det DT pobj NN punct . \n",
      "det DT compound NNP nsubjpass NNP auxpass VBZ ROOT VBN agent IN pobj NNS prep IN pobj NNS det DT npadvmod NN punct . \n",
      "det DT nsubjpass NN auxpass VBD ROOT VBN agent IN det DT pobj NNS aux TO xcomp VB nsubj PRP ccomp VB punct . \n",
      "det DT nsubjpass NN aux MD auxpass VB ROOT VBN agent IN det DT pobj NN npadvmod NN punct . \n",
      "det DT compound NN nsubjpass NNS auxpass VBD advmod RB ROOT VBN agent IN det DT pobj NN punct . \n",
      "det DT nsubjpass NN auxpass VBD ROOT VBN agent IN det DT pobj NN prep IN poss PRP$ pobj NN punct . \n",
      "det DT amod JJ nsubjpass NN auxpass VBD ROOT VBN agent IN pobj WP punct . \n",
      "66\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "Active = \"Harry ate six shrimp at dinner.\\\n",
    "Beautiful giraffes roam the savannah.\\\n",
    "Sue changed the flat tire.\\\n",
    "We are going to watch a movie tonight.\\\n",
    "I ran the obstacle course in record time.\\\n",
    "The crew paved the entire stretch of highway.\\\n",
    "Mom read the novel in one day.\\\n",
    "The critic wrote a scathing review.\\\n",
    "I will clean the house every Saturday.\\\n",
    "The staff is required to watch a safety video every year.\\\n",
    "She faxed her application for a new job.\\\n",
    "Tom painted the entire house.\\\n",
    "The teacher always answers the students’ questions.\\\n",
    "The choir really enjoys that piece.\\\n",
    "Who taught you to ski?\\\n",
    "The forest fire destroyed the whole suburb.\\\n",
    "The two kings are signing the treaty.\\\n",
    "The cleaning crew vacuums and dusts the office every night.\\\n",
    "Larry generously donated money to the homeless shelter.\\\n",
    "No one responded to my sales ad.\\\n",
    "The wedding planner is making all the reservations.\\\n",
    "Susan will bake two dozen cupcakes for the bake sale.\\\n",
    "The science class viewed the comet.\\\n",
    "Who ate the last cookie?\\\n",
    "Alex posted the video on Facebook.\\\n",
    "The director will give you instructions.\\\n",
    "Thousands of tourists view the Grand Canyon every year.\\\n",
    "The homeowners remodeled the house to help it sell.\\\n",
    "The team will celebrate their victory tomorrow.\\\n",
    "The saltwater eventually corroded the metal beams.\\\n",
    "The kangaroo carried her baby in her pouch.\\\n",
    "Some people raise sugar cane in Hawaii.\\\n",
    "He buys a camera.\\\n",
    "She drinks water.\\\n",
    "I know him.\\\n",
    "Water fills a tub.\\\n",
    "Women are not treated as equals.\\\n",
    "John has to study all afternoon.\\\n",
    "John is a good student.\\\n",
    "The dragon has scorched the metropolis with his fiery breath.\\\n",
    "After suitors invaded her house, Penelope had to think of ways to delay her remarriage.\\\n",
    "The Lao People’s Revolutionary Party set up a new system of drug control laws.\\\n",
    "Research points to heart disease as the leading cause of death in the United States.\\\n",
    "The surgeon positions the balloon in an area of blockage and inflates it.\\\n",
    "James writes the letters.\\\n",
    "James wrote the letters.\\\n",
    "James is writing the letters.\\\n",
    "James has written the letters.\\\n",
    "James is going to write the letters.\\\n",
    "James will write the letters.\\\n",
    "James was writing the letters.\\\n",
    "The scientists had found the cure, but it was too late.\\\n",
    "The scientists will have found a cure by then.\\\n",
    " I keep the butter in the fridge.\\\n",
    "John is keeping my house tidy.\\\n",
    "Mary kept her schedule meticulously.\\\n",
    "The theater was keeping a seat for you.\\\n",
    "I have kept all your old letters.\\\n",
    "He had kept up his training regimen for a month.\\\n",
    "Mark will keep the ficus.\\\n",
    "If you told me, I would keep your secret.\\\n",
    "I would have kept your bicycle here if you had left it with me.\\\n",
    "She wants to keep the book.\\\n",
    "Judy was happy to have kept the puppy.\\\n",
    "I have a feeling that you may be keeping a secret.\\\n",
    "Having kept the bird in a cage for so long, Jade wasn't sure it could survive in the wild.\\\n",
    "Guests might not play chess.\\\n",
    "He might meet Dewi.\\\n",
    "Dewi must not open the gate every morning.\\\n",
    "He must finish his duty in a week.\\\n",
    "I may not buy the computer.\\\n",
    "He may sell the house.\\\n",
    "May I buy the computer?\\\n",
    "Risky can not buy this car every time.\\\n",
    "She can sell the car every time.\\\n",
    "Can she play a violin?\"\n",
    "\n",
    "Passive = \"A camera is bought by him.\\\n",
    "Water is drunk by her.\\\n",
    "He is known to me.\\\n",
    "A tub is filled with water.\\\n",
    "Sugar is sold in kilograms.\\\n",
    "There is a considerable range of expertise demonstrated by the spam senders.\\\n",
    "It was determined by the committee that the report was inconclusive.\\\n",
    "We were invited by our neighbors to attend their party.\\\n",
    "Groups help participants realize that most of their problems and secrets are shared by others in the group.\\\n",
    "The proposed initiative will be bitterly opposed by abortion rights groups.\\\n",
    "Minor keys, modal movement, and arpeggios are shared by both musical traditions.\\\n",
    "In this way, the old religion was able to survive the onslaught of new ideas until the old gods were finally displaced by Christianity.\\\n",
    "First the apples are picked, then they are cleaned, and finally they’re packed and shipped to the market.\\\n",
    "New York is considered the most diverse city in the U.S.\\\n",
    "It is believed that Amelia Earhart’s plane crashed in Pacific Ocean.\\\n",
    "Hungarian is seen as one of the world’s most difficult languages to learn.\\\n",
    "Skin cancers are thought to be caused by excessive exposure to the sun.\\\n",
    "George Washington was elected president in 1788.\\\n",
    "Two people were killed in a drive-by shooting on Friday night.\\\n",
    "Ten children were injured when part of the school roof collapsed.\\\n",
    "I was hit by the dodgeball.\\\n",
    "The metropolis has been scorched by the dragon’s fiery breath.\\\n",
    "When her house was invaded, Penelope had to think of ways to delay her remarriage.\\\n",
    "A new system of drug control laws was set up.\\\n",
    "Heart disease is considered the leading cause of death in the United States.\\\n",
    "The balloon is positioned in an area of blockage and is inflated.\\\n",
    "The Exxon Company accepts that a few gallons might have been spilled.\\\n",
    "100 votes are required to pass the bill.\\\n",
    "Over 120 different contaminants have been dumped into the river.\\\n",
    "Baby Sophia was delivered at 3:30 a.m. yesterday.\\\n",
    "The letters are written by James.\\\n",
    "The letters were written by James.\\\n",
    "The letters are being written by James.\\\n",
    "The letters have been written by James.\\\n",
    "The letters are going to be written by James.\\\n",
    "The letters will be written by James.\\\n",
    "The letters were being written by James.\\\n",
    "The cure had been found, but it was too late.\\\n",
    "A cure will have been found by then.\\\n",
    "Mistakes were made.\\\n",
    "The butter is kept in the fridge.\\\n",
    "My house is being kept tidy.\\\n",
    "Mary's schedule was kept meticulously.\\\n",
    "A seat was being kept for you.\\\n",
    "All your old letters have been kept.\\\n",
    "His training regimen had been kept up for a month.\\\n",
    "The ficus will be kept.\\\n",
    "If you told me, your secret would be kept.\\\n",
    "Your bicycle would have been kept here if you had left it with me.\\\n",
    "The book wants to be kept.\\\n",
    "The puppy was happy to have been kept.\\\n",
    "I have a feeling that a secret may be being kept.\\\n",
    "The bird, having been kept in a cage for so long, might not survive in the wild.\\\n",
    "The car can be sold by her every time.\\\n",
    "Can a violin be played by her?\\\n",
    "The house may be sold by him.\\\n",
    "May the computer be bought by me?\\\n",
    "The computer may not be bought by me.\\\n",
    "His duty must be finished by him in a week.\\\n",
    "The gate must not be opened by Dewi every morning.\\\n",
    "Dewi might be met by him.\\\n",
    "Chess might not be played guests.\\\n",
    "At dinner, six shrimp were eaten by Harry.\\\n",
    "The savannah is roamed by beautiful giraffes.\\\n",
    "The flat tire was changed by Sue.\\\n",
    "A movie is going to be watched by us tonight.\\\n",
    "The obstacle course was run by me in record time.\\\n",
    "The entire stretch of highway was paved by the crew.\\\n",
    "The novel was read by Mom in one day.\\\n",
    "A scathing review was written by the critic.\\\n",
    "The house will be cleaned by me every Saturday.\\\n",
    "A safety video will be watched by the staff every year.\\\n",
    "The application for a new job was faxed by her.\\\n",
    "The entire house was painted by Tom.\\\n",
    "The students’ questions are always answered by the teacher.\\\n",
    "That piece is really enjoyed by the choir.\\\n",
    "By whom were you taught to ski?\\\n",
    "The whole suburb was destroyed by the forest fire.\\\n",
    "The treaty is being signed by the two kings.\\\n",
    "Every night the office is vacuumed and dusted by the cleaning crew.\\\n",
    "Money was generously donated to the homeless shelter by Larry.\\\n",
    "My sales ad was not responded to by anyone.\\\n",
    "All the reservations will be made by the wedding planner.\\\n",
    "For the bake sale, two dozen cookies will be baked by Susan.\\\n",
    "The comet was viewed by the science class.\\\n",
    "The video was posted on Facebook by Alex.\\\n",
    "Instructions will be given to you by the director.\\\n",
    "The Grand Canyon is viewed by thousands of tourists every year.\\\n",
    "The house was remodeled by the homeowners to help it sell.\\\n",
    "The victory will be celebrated by the team tomorrow.\\\n",
    "The metal beams were eventually corroded by the saltwater.\\\n",
    "The baby was carried by the kangaroo in her pouch.\\\n",
    "The last cookie was eaten by whom?\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "text = Passive\n",
    "doc = nlp(text)\n",
    "sents = list(doc.sents)\n",
    "print(\"Number of Sentences = \",len(sents))\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.dep_,token.tag_, end = \" \")\n",
    "    print()\n",
    "passive_rule = [{'DEP':'nsubjpass'},{'DEP':'aux','OP':'*'},{'DEP':'auxpass'},{'TAG':'VBN'}]\n",
    "matcher.add('Passive',None,passive_rule)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Emoji\n",
    "\n",
    "Using the python package \"spacymoji\"\n",
    ">`pip install spacymoji`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"assert doc[2:5]._.has_emoji == True\\nassert doc[0]._.is_emoji == False\\nassert doc[4]._.is_emoji == True\\nassert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\\nassert len(doc._.emoji) == 2\\nassert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "emoji = Emoji(nlp)\n",
    "#nlp.add_pipe(emoji, first=True)\n",
    "doc = nlp(u\"This is a test 😻 👍🏿\")\n",
    "#assert doc._.has_emoji == True\n",
    "\"\"\"assert doc[2:5]._.has_emoji == True\n",
    "assert doc[0]._.is_emoji == False\n",
    "assert doc[4]._.is_emoji == True\n",
    "assert doc[5]._.emoji_desc == u'thumbs up dark skin tone'\n",
    "assert len(doc._.emoji) == 2\n",
    "assert doc._.emoji[1] == (u'👍🏿', 5, u'thumbs up dark skin tone')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Using an LSTM sentiment classification model trained using Keras in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3fefb19146bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcytoolz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example shows how to use an LSTM sentiment classification model trained\n",
    "using Keras in spaCy. spaCy splits the document into sentences, and each\n",
    "sentence is classified using the LSTM. The scores for the sentences are then\n",
    "aggregated to give the document score. This kind of hierarchical model is quite\n",
    "difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras\n",
    "example on this dataset performs quite poorly, because it cuts off the documents\n",
    "so that they're a fixed size. This hurts review accuracy a lot, because people\n",
    "often summarise their rating in the final sentence\n",
    "\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        with (path / \"config.json\").open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / \"model\").open(\"rb\") as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype=\"int32\")\n",
    "\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype=\"int32\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    dev_texts,\n",
    "    dev_labels,\n",
    "    lstm_shape,\n",
    "    lstm_settings,\n",
    "    lstm_optimizer,\n",
    "    batch_size=100,\n",
    "    nb_epoch=5,\n",
    "    by_sentence=True,\n",
    "):\n",
    "\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape[\"max_length\"])\n",
    "    dev_X = get_features(dev_docs, lstm_shape[\"max_length\"])\n",
    "    model.fit(\n",
    "        train_X,\n",
    "        train_labels,\n",
    "        validation_data=(dev_X, dev_labels),\n",
    "        epochs=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape[\"max_length\"],\n",
    "            trainable=False,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True,\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape[\"nr_hidden\"], use_bias=False)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                shape[\"nr_hidden\"],\n",
    "                recurrent_dropout=settings[\"dropout\"],\n",
    "                dropout=settings[\"dropout\"],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(shape[\"nr_class\"], activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=settings[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    for subdir, label in ((\"pos\", 1), (\"neg\", 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\n",
    "    if limit >= 1:\n",
    "        examples = examples[:limit]\n",
    "    return zip(*examples)  # Unzips into two lists\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    train_dir=(\"Location of training file or directory\"),\n",
    "    dev_dir=(\"Location of development file or directory\"),\n",
    "    model_dir=(\"Location of output model directory\",),\n",
    "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
    "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
    "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
    "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
    "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
    "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
    "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
    "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(\n",
    "    model_dir=None,\n",
    "    train_dir=None,\n",
    "    dev_dir=None,\n",
    "    is_runtime=False,\n",
    "    nr_hidden=64,\n",
    "    max_length=100,  # Shape\n",
    "    dropout=0.5,\n",
    "    learn_rate=0.001,  # General NN config\n",
    "    nb_epoch=5,\n",
    "    batch_size=256,\n",
    "    nr_examples=-1,\n",
    "):  # Training params\n",
    "    if model_dir is not None:\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if train_dir is None or dev_dir is None:\n",
    "        imdb_data = thinc.extra.datasets.imdb()\n",
    "    if is_runtime:\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir)\n",
    "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        if train_dir is None:\n",
    "            train_texts, train_labels = zip(*imdb_data[0])\n",
    "        else:\n",
    "            print(\"Read data\")\n",
    "            train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)\n",
    "        train_labels = numpy.asarray(train_labels, dtype=\"int32\")\n",
    "        dev_labels = numpy.asarray(dev_labels, dtype=\"int32\")\n",
    "        lstm = train(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            dev_texts,\n",
    "            dev_labels,\n",
    "            {\"nr_hidden\": nr_hidden, \"max_length\": max_length, \"nr_class\": 1},\n",
    "            {\"dropout\": dropout, \"lr\": learn_rate},\n",
    "            {},\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / \"model\").open(\"wb\") as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / \"config.json\").open(\"w\") as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "length is medium\n",
      "length is medium\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "length is medium\n",
      "length is high\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "length is medium\n",
      "length is medium\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "length is medium\n",
      "lenght is low\n",
      "lenght is low\n",
      "lenght is low\n",
      "length is medium\n",
      "length is medium\n",
      "lenght is low\n",
      "length is high\n",
      "lenght is low\n",
      "length is medium\n",
      "length is medium\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "myfile=(open('examplefile.txt' ,encoding=\"utf8\")).read()\n",
    "doc_file=nlp(myfile)\n",
    "def calcVerb(sente):\n",
    "    c=0\n",
    "    for word in sente:\n",
    "        if(word.pos_=='VERB'):\n",
    "            c=c+1\n",
    "    if(c<3):\n",
    "        print(\"lenght is low\")\n",
    "    elif(c<6):\n",
    "        print(\"length is medium\")\n",
    "    else:\n",
    "        print(\"length is high\")\n",
    "for num,sent in enumerate(doc_file.sents):\n",
    "    calcVerb(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
